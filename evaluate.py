import tensorflow as tf
import numpy as np

import utils
import model
from hyperparameters import hps


def evaluate(c_model, g_models):
    real_image_ds = utils.get_dataset(False)

    distance = utils.TVDistance()
    fid = utils.FIDistance()
    for (real_imgs, _) in real_image_ds:

        rvs = tf.random.normal(shape=(hps.num_gens * hps.batch_size//hps.num_gens, hps.noise_dim))
        rvs = tf.split(rvs, hps.num_gens, axis=0)
        fake_images = tf.nest.map_structure(
            lambda rv, gen_i: gen_i.predict(rv),
            rvs, g_models
        )
        fake_images = tf.concat(fake_images, axis=0)
        fake_logits = c_model.predict(fake_images)
        fake_dists = tf.nn.softmax(fake_logits)
        fake_dists_list = tf.split(fake_dists, hps.num_gens, axis=0)

        fid.update_state(real_imgs, fake_images)
        distance.update_state(fake_dists_list[1], fake_dists_list[0])

    return fid.result(), distance.result()


def save_im(g_models):

    rvs = tf.random.normal(shape=(hps.num_gens*hps.batch_size, hps.noise_dim))
    rvs = tf.split(rvs, hps.num_gens, axis=0)

    fake_images = tf.nest.map_structure(
        lambda rv, gen_i: gen_i.predict(rv),
        rvs, g_models
    )
    fake_images = tf.concat(fake_images, axis=0)
    generated_imgs = (fake_images * 127.5) + 127.5
    for i in range(hps.num_gens*hps.batch_size):
        img = generated_imgs[i]
        img = tf.keras.preprocessing.image.array_to_img(img)
        filename = "gen/{i}.png".format(i=i)
        filename = hps.gen_img_dir + filename
        img.save(filename)


def average_class_probs(g_models, c_model):
    rvs = tf.random.normal(shape=(hps.num_gens * hps.batch_size, hps.noise_dim))
    rvs = tf.split(rvs, hps.num_gens, axis=0)

    fake_images = tf.nest.map_structure(
        lambda rv, gen_i: gen_i.predict(rv),
        rvs, g_models
    )
    classes = tf.nest.map_structure(
        lambda batch: tf.nn.softmax(c_model.predict(batch)),
        fake_images
    )
    classes0 = tf.reduce_mean(classes[0], axis=0)
    classes1 = tf.reduce_mean(classes[1], axis=0)

    return classes0, classes1

if __name__ == "__main__":
    np.set_printoptions(suppress=True)
    tf.keras.backend.clear_session()

    classifier = model.Classifier()
    classifier.build((None, 32, 32, 1))
    classifier.load_weights(hps.savedir + "classifier" + ".h5")

    generators = []
    for i in range(hps.num_gens):
        gen = model.Generator(i)
        gen.build((None, hps.noise_dim))
        gen.load_weights(hps.savedir + "gen{}".format(i) + ".h5")
        generators.append(gen)

    FID, dist = evaluate(classifier, generators)
    save_im(generators)

    print("Generator mean FID = {}".format(FID))
    print("Generators' image distance = {}".format(dist) + " (TVD)")

    c0, c1 = average_class_probs(generators, classifier)
    print("Average class probability generated by gen0(%):")
    print(c0.numpy()*100)
    print("Average class probability generated by gen1(%):")
    print(c1.numpy()*100)
